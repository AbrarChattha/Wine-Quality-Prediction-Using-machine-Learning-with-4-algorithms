{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('winequality-red.csv', low_memory=False, sep=';')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_col_spaces(wine_set):\n",
    "    wine_set.columns = [x.strip().replace(' ', '_') for x in wine_set.columns]\n",
    "    return wine_set\n",
    "\n",
    "remove_col_spaces(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# split into input (X) and output (Y) variables, splitting csv dat\n",
    "x_train,x_test,y_train,y_test,=train_test_split(df[['fixed_acidity','volatile_acidity','citric_acid','residual_sugar','chlorides','free_sulfur_dioxide','total_sulfur_dioxide','density','pH','sulphates','alcohol']],df['quality'],test_size=0.2,random_state=100,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model, add dense layers one by one specifying activation function\n",
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim=11, activation='relu')) # input layer requires input_dim param\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(1, activation='sigmoid')) # sigmoid instead of relu for final probability between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1279 samples, validate on 320 samples\n",
      "Epoch 1/10\n",
      "1279/1279 [==============================] - 1s 1ms/step - loss: -73.3201 - accuracy: 0.0000e+00 - val_loss: -133.6246 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1279/1279 [==============================] - 0s 153us/step - loss: -266.0286 - accuracy: 0.0000e+00 - val_loss: -461.7687 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1279/1279 [==============================] - 0s 133us/step - loss: -871.0885 - accuracy: 0.0000e+00 - val_loss: -1446.9136 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1279/1279 [==============================] - 0s 158us/step - loss: -2516.5789 - accuracy: 0.0000e+00 - val_loss: -3895.1525 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1279/1279 [==============================] - 0s 129us/step - loss: -6067.0335 - accuracy: 0.0000e+00 - val_loss: -8939.3628 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1279/1279 [==============================] - 0s 134us/step - loss: -13460.6318 - accuracy: 0.0000e+00 - val_loss: -18299.1306 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1279/1279 [==============================] - 0s 134us/step - loss: -25629.6140 - accuracy: 0.0000e+00 - val_loss: -33566.4945 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1279/1279 [==============================] - 0s 147us/step - loss: -44686.9886 - accuracy: 0.0000e+00 - val_loss: -56651.4861 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1279/1279 [==============================] - 0s 155us/step - loss: -72493.1227 - accuracy: 0.0000e+00 - val_loss: -89835.5273 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1279/1279 [==============================] - 0s 130us/step - loss: -113724.1227 - accuracy: 0.0000e+00 - val_loss: -135959.6704 - val_accuracy: 0.0000e+00\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# call the function to fit to the data (training the network)\n",
    "history=model.fit(x_train, y_train, epochs = 10, batch_size=20, validation_data=(x_test, y_test))\n",
    "\n",
    "print(model.predict(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 1279 input samples and 320 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-7913ab6708d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n%s: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1350\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                 \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_array_length_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m                 \u001b[1;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    242\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    245\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 1279 input samples and 320 target samples."
     ]
    }
   ],
   "source": [
    "scores=model.evaluate(x_train,y_test)\n",
    "\n",
    "print(\"\\n%s: %.2f%%\" %(model.metrics_names[1],scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
